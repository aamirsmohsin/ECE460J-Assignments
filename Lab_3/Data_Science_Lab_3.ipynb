{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1yW-a8mXkz0"
   },
   "source": [
    "### Group Members: Weston Lu, Kevin Wang, Aamir Mohsin\n",
    "# Data Science Lab: Lab 3\n",
    "\n",
    "Submit:\n",
    "1. A pdf of your notebook with solutions. Make sure that the solutions are present and visible in the pdf.\n",
    "2. A link to your colab notebook or also upload your .ipynb if not working on colab.\n",
    "\n",
    "# Goals of this Lab\n",
    "\n",
    "1. More experience with regression and ridge regression (regularization)\n",
    "2. Start playing with Kaggle\n",
    "3. More experience with Lasso.\n",
    "4. An initial shot at ensembling and stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eUrt5BJXeSE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHpsPgLkXr4y"
   },
   "source": [
    "## Problem 1 (Optional)\n",
    "\n",
    "**Part 1** Make sure you can run through and understand the Jupyter notebook on Ridge\n",
    "Regression and Colinearity we saw in class: https://colab.research.google.com/drive/1R7xTNHxAwhL1tANiGT2KRO-OT0D8KV2Z\n",
    "\n",
    "**Part 2.** What is the test error of the “zero-variance” solution, namely, the all-zeros solution?\n",
    "\n",
    "**Part 3.** The least-squares solution does not seem to do too well, because it has so much variance. Still, it is unbiased. Show this empirically: generate many copies of the data, and for each one, obtain the least-squares solution. Average these, to show that while each run produces a beta hat that is very different, their average begins to look more and more like the true beta.\n",
    "\n",
    "**Part 4.** Alternatively, if one had access to lots of data, instead of computing the least-square solution over smaller batches and then averaging these solutions as in the previous part of the problem, an approach is to run a single least-squares regression over all the data. Which approach do you think is better? Can you support your conclusion with experiments?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8oqAWL5YbOn"
   },
   "source": [
    "### Problem 2: Starting in Kaggle.\n",
    "Later this semester, we are opening a Kaggle competition made for this class. In that one, you will be participating on your own. This is an intro to get us started, and also an excuse to work with regularization and regression which we have been discussing.\n",
    "\n",
    "**Part 1.** Let’s start with our first Kaggle submission in a playground regression competition. Make an account to Kaggle and find https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "\n",
    "**Part 2.** Follow the data preprocessing steps from (new link!) https://www.kaggle.com/code/apapiu/regularized-linear-models. Then run a ridge regression using $\\lambda = 0.1$. Make a submission of this prediction, what is the RMSE you get? (Hint: remember to exponentiate np.expm1(ypred) your predictions).\n",
    "\n",
    "\n",
    "\n",
    "**Part 3.** Compare a ridge regression and a lasso regression model. Optimize the alphas using cross validation. What is the best score you can get from a single ridge regression model and from a single lasso model?\n",
    "\n",
    "**Part 4.** The $\\ell_0$ (or $L_0$) norm is the number of nonzeros of a vector. Plot the $L_0$ norm of the coefficients that lasso produces as you vary the strength of regularization parameter $\\lambda$.\n",
    "\n",
    "**Part 5.** Add the outputs of your models as features and train a ridge regression on all the features plus the model outputs (This is called Ensembling and Stacking). Be careful not to overfit. What score can you get? (We will be discussing ensembling more, later in the class, but you can start playing with it now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUkEWo-g3CQm"
   },
   "source": [
    "## Problem 3 (Nothing to turn in)\n",
    "\n",
    "Run this simple example from scikit learn, and understand what each command is doing: https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to4_4HOv3Fm3"
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "Use the data generation used in the LASSO notebook where we first introduced Lasso, to generate data.\n",
    "\n",
    "You can find that again here: https://colab.research.google.com/drive/1_NGlKLpXpcobUIlan5DY5nA-5aT39Hxc\n",
    "\n",
    "**Part 1.** Manually implement forward selection. Report the order in which you add features.\n",
    "\n",
    "**Part 2.** In this example, we know the true support size is 5. But what if we did not know this? Plot test error as a function of the size of the support. Use this to recover the true support size. Justify your answer.\n",
    "\n",
    "**Part 3.** Use Lasso with a manually implemented Cross validation using the metric of your choice.\n",
    "What is the value of the hyperparameter? (Manually implemented means that you can either\n",
    "do it entirely on your own, or you can use GridSearchCV, but I’m asking you not to use\n",
    "LassoCV, which you will use in the next problem).\n",
    "\n",
    "**Part 4.** (Optional) Change the number of folds in your CV and repeat the previous step. How does the optimal\n",
    "value of the hyperparameter change? Try to explain any trends that you find.\n",
    "\n",
    "**Part 5.** (Optional) Read about and use LassoCV from sklearn.linear model. How does this compare with what\n",
    "you did in the previous step? If they agree, then explain why they agree, and if they disagree\n",
    "explain why. This will require you to make sure you understand what LassoCV is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F02CHDUH3LGG"
   },
   "source": [
    "## Problem 5 (Optional): Higher vs Lower K in K-Fold CV.\n",
    "\n",
    "Using either Ridge regression (e.g., with the setting in the Ridge Regression colab notebook) or Lasso (e.g., the setting of the Lasso colab notebook, also linked to above), or with any other data sets you wish to construct, design and execute an experiment to investigate the claim when we do $k$-fold cross validation, as $k$ decreases, we have more bias but less variance.  Note that this is an open-ended exercise. It is asking you to use simulation and investigate what is going on with increasing or decreasing the number of folds in cross validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp_2W_f63MxT"
   },
   "source": [
    "## Problem 6 (Optional) Elastic Net\n",
    "\n",
    "There may be settings where we want to combine ideas from Ridge and Lasso. There is a model that does this, by adding an L1 penalty (as in Lasso) and also an L2 penalty (as in Ridge). Read about this in sklearn and in [ISL](https://www.statlearning.com/) (or anywhere else). Try to construct an example where ElasticNetCV does better than LassoCV. Explain how you came up with this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_bYe1xfYdBB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{"cells":[{"cell_type":"markdown","metadata":{"id":"vMQXDZXXN8q2"},"source":["# Data Science Lab: Lab 6\n","\n","Submit:\n","\n","A pdf of your notebook with solutions.\n","A link to your colab notebook or also upload your .ipynb if not working on colab.\n","\n","# Goals of this Lab\n","\n","**Fully Connected Models and XOR**\n","\n","\n","1. How to create data objects that pytorch can use\n","2. How to create a dataloader\n","3. How to define a basic fully connected single layer model\n","4. How to define a multi-layer fully connected model\n","5. How to add non-linear activation functions.\n","6. How to add layers in two different ways\n","\n","We also see the importance of nonlinear activation functions directly, by experimenting with the simple 4-data-point XOR example that we saw in class.\n","\n","We also see that neural networks are fundamentally different than, say, logistic regression or linear regression, in that they can get stuck in local minima. A simple way to say this is that a different random seed may lead to a different solution.\n","\n","We explore how capacity (the size of the neural network) could affect this situation.\n","\n","**Note:** Make sure that you fix a random seed so that you can replicate your solutions. This is always a good practice, and in particular important here, as we see below."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"T6k-hagH8H0l"},"outputs":[],"source":["import torch\n","import numpy as np\n","import time\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"-LLf1qGBkNx4"},"source":["# First we define a linear regressor.\n","This is the same as a fully connected layer. It will be a building block in making deeper neural networks with fully connected layers."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5ikI4E7WXrCQ"},"outputs":[],"source":["# We define our first class: LinearRegressor\n","#\n","class LinearRegressor(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Define the layer(s) needed for the linear model.\n","        \"\"\"\n","        super().__init__()\n","        self.linear = torch.nn.Linear(input_dim, output_dim, bias = True) # just linear\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Calculate the regression score (MSE).\n","\n","        Input:\n","            x (float tensor N x d): input rows\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.linear(x)\n","        return torch.flatten(x)\n","\n","\n","    # defining a separate predict function is useful for multi-class\n","    # classification as we will see later. Here it is\n","    # unnecessary.\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict the regression label of the input vector.\n","\n","        Input:\n","            x (float tensor N X d): input images\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.linear(x)\n","        return torch.flatten(x)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"36zhuTCUkZXT"},"source":["## Problem 1:\n","\n","Now you will use torch.nn.Sequential (see https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to construct a two layer neural network, with two fully connected layers (no non-linearity yet). Thus, you will combine torch.nn.Sequential with torch.nn.Linear that you saw above.\n","\n","\n","Design your network so that the first layer has as many neurons as the input.\n","\n","Note: you have only one line to fill in here."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0LpZdyIOjlr4"},"outputs":[],"source":["class TwoLayerLinearRegressor(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Define a model that stacks two linear fully connected layers.\n","        \"\"\"\n","        super().__init__()\n","        self.TLL = torch.nn.Sequential(\n","            torch.nn.Linear(input_dim, input_dim),\n","            torch.nn.Linear(input_dim, output_dim)\n","        )# TO DO: complete this equality, using torch.nn.Sequential\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Calculate the regression score (MSE).\n","\n","        Input:\n","            x (float tensor N x d): input rows\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.TLL(x)\n","        return torch.flatten(x)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict the regression label of the input vector.\n","\n","        Input:\n","            x (float tensor N X d): input images\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.TLL(x)\n","        return torch.flatten(x)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A9JRe7K3lQx7"},"source":["## Problem 2\n","\n","Now you will create the same network, but using different syntax: you will not use torch.nn.Sequential. You need to fill in the two lines as noted by the comments."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vZSpFBJdj-Da"},"outputs":[],"source":["class TwoLayerLinearRegressor2(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Define a model that stacks two linear fully connected layers.\n","        \"\"\"\n","        super().__init__()\n","        self.fc1 = torch.nn.Linear(input_dim, input_dim) # TO DO\n","        self.fc2 = torch.nn.Linear(input_dim, output_dim) # TO DO\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Calculate the regression score (MSE).\n","\n","        Input:\n","            x (float tensor N x d): input rows\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return torch.flatten(x)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict the regression label of the input vector.\n","\n","        Input:\n","            x (float tensor N X d): input images\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return torch.flatten(x)\n"]},{"cell_type":"markdown","metadata":{"id":"pTKDb01ilnks"},"source":["## Problem 3\n","\n","Now you will define a 2 layer neural network with ReLU activation at the first layer. In other words:\n","\n","Let $x$ be the input.\n","Then writing $z = Wx + c$, $h=$ReLU$(z)$ is the first layer's neurons. Then the output is $y = w\\cdot z+d$.\n","\n","Create this neural network using the torch.nn.Sequential command. Conceptually, it may help to realize that this neural network is: a fully connected layer followed by a ReLU, followed by a fully connected layer.\n","\n","Also see: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n","\n","Note: You have only one line to fill in here."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"031WUqCajrJ_"},"outputs":[],"source":["class TwoLayerNonLinearRegressor(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dim = 15):\n","        \"\"\"\n","        Define a model that has a linear layer, a ReLU layer and another linear layer.\n","        \"\"\"\n","        super().__init__()\n","        self.linear = torch.nn.Sequential(\n","            torch.nn.Linear(input_dim, hidden_dim),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(hidden_dim, output_dim)\n","        )# TO DO\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Calculate the regression score (MSE).\n","\n","        Input:\n","            x (float tensor N x d): input rows\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.linear(x)\n","        return torch.flatten(x)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict the regression label of the input vector.\n","\n","        Input:\n","            x (float tensor N X d): input images\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.linear(x)\n","        return torch.flatten(x)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ApkXVRwfmaXj"},"source":["## Problem 4\n","\n","Do this one more time, but now without torch.nn.Sequential.\n","\n","You have three lines to fill in here."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"W7CrOFr8jtdp"},"outputs":[],"source":["# We now do this again, without using nn.sequential\n","# in order to illustrate different syntax.\n","\n","class TwoLayerNonLinearRegressor2(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Define a model that has a linear layer, a ReLU layer and another linear layer.\n","        \"\"\"\n","        super().__init__()\n","        self.fc1 = torch.nn.Linear(input_dim, input_dim) # TO DO\n","        self.relu = torch.nn.ReLU() # TO DO\n","        self.fc2 = torch.nn.Linear(input_dim, output_dim) # TO DO\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Calculate the regression score (MSE).\n","\n","        Input:\n","            x (float tensor N x d): input rows\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return torch.flatten(x)\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict the regression label of the input vector.\n","\n","        Input:\n","            x (float tensor N X d): input images\n","        Output:\n","            y (float tensor N x 1): regression output\n","        \"\"\"\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return torch.flatten(x)"]},{"cell_type":"markdown","metadata":{"id":"7-7v8qRLmk3b"},"source":["## Problem 5 (Nothing to turn in)\n","\n","Read the documentation https://pytorch.org/docs/stable/optim.html to see what are the options pytorch provides for an optimizer, and what the parameters are."]},{"cell_type":"markdown","metadata":{"id":"-oYjzlVqve2K"},"source":["## Problem 6\n","\n","Explain what the code\n","\n","\n","```\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","```\n","and\n","\n","\n","```\n","model.to(device)\n","x = x.to(device)\n","y = y.to(device)\n","```\n","\n","does. This does not have to be a completely rigorous definition -- just explain enough to make clear that you understand what the code is doing and why it's important.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The first line checks if there is a CUDA-enabled GPU on the device and will run subsequent models using that if there is, otherwise it will just use the CPU. GPU's often speed up training with parallelism so it's beneficial to use it if available. The following lines moves the model's parameters to the specified device as well as the input data X and labels y."]},{"cell_type":"markdown","metadata":{"id":"6r3mmQ0y5JxO"},"source":["## Problem 7\n","\n","Modify the code below for the training loop, so that in addition to training, you output a curve of the training error over each epoch."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-gYODAFizPe-"},"outputs":[],"source":["# Now we define a function for training\n","# Note each of the arguments that it takes\n","import matplotlib.pyplot as plt\n","\n","#TODO: add a plot of the training error over each epoch\n","\n","def train(model, data_train, data_val, device, lr=0.01, epochs=5000):\n","    \"\"\"\n","    Train the model.\n","\n","    Input:\n","      model (torch.nn.Module): the model to train\n","      data_train (torch.utils.data.Dataloader): yields batches of data\n","      data_val (torch.utils.data.Dataloader): use this to validate your model\n","      device (torch.device): which device to use to perform computation\n","\n","      (optional) lr: learning rate hyperparameter\n","      (optional) epochs: number of passes over dataloader\n","    \"\"\"\n","\n","    # Setup the loss function to use: mean squared error\n","    loss_function = torch.nn.MSELoss(reduction = 'sum')\n","\n","    # Setup the optimizer -- just generic ADAM\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    train_losses = []\n","\n","    # Wrap in a progress bar.\n","    for epoch in tqdm(range(epochs)):\n","        # Set the model to training mode.\n","        model.train()\n","        train_loss = 0\n","\n","        for x, y in data_train:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            # Forward pass through the network\n","            output = model(x)\n","\n","            # Compute loss\n","            loss = loss_function(output, y)\n","            # losses.append(loss)\n","            # update model weights.\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        train_loss /= len(data_train)\n","        train_losses.append(train_loss)\n","\n","        # Set the model to eval mode and compute accuracy.\n","        model.eval()\n","\n","        accuracys_val = list()\n","\n","        for x, y in data_val:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            y_pred = model.predict(x)\n","\n","    \n","    # Plotting the array vs its index\n","    # plt.plot(train_losses)  # Plot the array\n","    # plt.xlabel('# of Epochs')\n","    # plt.ylabel('Training Error')\n","    # plt.title('Training Error vs. Num Epochs')\n","    #     # plt.grid(True)  # Optional: adds gridlines for better readability\n","    # plt.show()\n","\n","    "]},{"cell_type":"code","execution_count":8,"metadata":{"id":"FObDOykPz16p"},"outputs":[],"source":["# We write a function that takes a model, evaluate on the validation\n","# data set and returns the predictions\n","\n","def evaluate_model(model,data_val,device):\n","  model.eval()\n","  output_vals = list()\n","  accuracys_val = list()\n","  for x, y in data_val:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            y_pred = model.predict(x)\n","            output_vals.append(y_pred)\n","            # accuracy_val = (y_pred == y).float().mean().item()\n","            # accuracys_val.append(accuracy_val)\n","\n","  # accuracy = torch.FloatTensor(accuracys_val).mean().item()\n","  return output_vals"]},{"cell_type":"markdown","metadata":{"id":"JtZYmSP5m7rT"},"source":["## Problem 8 (Nothing to turn in)\n","\n","Read the documentation and try to understand what a dataloader is. You can start here https://pytorch.org/docs/stable/data.html but there are many tutorials out there as well."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"qtVUvfryzIJC"},"outputs":[{"name":"stdout","output_type":"stream","text":["These are the labels:\n"," [ 0.02984931 -2.37607964 -1.10216844 -0.0546588   2.05416286  2.26763019\n","  2.12402512 -4.33808936  5.42576936  1.55617834  1.47426999 -1.17686426\n"," -4.17001831 -4.92588359 -3.1593322 ]\n","These are the features:\n"," [[ 2.18436371  1.09668251 -0.52891594]\n"," [-0.73117871  0.27524319 -0.68482887]\n"," [-0.79212898  0.4478784   0.06891947]\n"," [ 0.85652464  0.32284947 -0.29416698]\n"," [-0.26769272 -0.99100096  0.66542731]\n"," [-1.10604711 -0.04052201  1.66657765]\n"," [ 2.02056113 -0.6863628  -0.2914494 ]\n"," [-2.04417592  1.67244    -0.31073672]\n"," [ 1.29268932 -1.98868467  1.07219768]\n"," [ 0.46026213 -1.73987115 -0.32197747]\n"," [ 2.44901268 -1.02318085 -0.99896177]\n"," [-0.66561075 -0.03847345 -0.27486348]\n"," [-0.90257166  0.70993038 -1.27875813]\n"," [ 0.34187719  1.52946229 -1.86914924]\n"," [ 0.18972835  0.46676165 -1.44114945]]\n"]}],"source":["# Creating the data: Linear Regression on Linear Data\n","from torch.utils.data import TensorDataset, DataLoader\n","N = 15\n","X = np.random.randn(N,3)\n","beta = np.array([1,-1,2])\n","Y = np.dot(X,beta)\n","tensor_x = torch.Tensor(X) # transform to torch tensor\n","tensor_y = torch.Tensor(Y)\n","print('These are the labels:\\n',Y)\n","print('These are the features:\\n',X)\n","\n","m = 1 # Batch size\n","data = TensorDataset(tensor_x,tensor_y) # create your datset\n","data_train = DataLoader(data,batch_size = m, shuffle = True) # create your dataloader with training data\n","data_val = DataLoader(data) # create your dataloader with validation data, here same as training"]},{"cell_type":"markdown","metadata":{"id":"aFTIV3OcnKS8"},"source":["## Now we train and evaluate the linear model."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"BdXFWEN41NBt"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2475cc027fc941c38015e09aaf3e0b64","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Define the model we wish to use, and train it.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = LinearRegressor(3, 1)\n","model.to(device)\n","\n","train(model, data_train, data_val, device)"]},{"cell_type":"markdown","metadata":{"id":"sw--ClBtnYHV"},"source":["## Here is some code for getting the parameters of the model."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"5hfTNlIjTNCV"},"outputs":[{"name":"stdout","output_type":"stream","text":["linear.weight tensor([[ 1.0000, -1.0000,  2.0000]])\n","linear.bias tensor([6.5703e-06])\n"]},{"data":{"text/plain":["'\\nfor name, param in model.named_parameters():\\n    if param.requires_grad:\\n        print (name, param.data)\\n'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Now let's get the model parameters.\n","# We can see that we have succeeded in learning beta: [1,-1,2]\n","for name, param in model.named_parameters():\n","  print (name, param.data)\n","\n","# If we wanted to, we could also only print the ones that we update (may be useful for more complex models)\n","\"\"\"\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print (name, param.data)\n","\"\"\""]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DwrvHtgIFbfl"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf66999cfbe94ea6851246f571349d58","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Now let's move to our second model: the two layer linear regressor.\n","# We again define the model using the class we created.\n","# Then we train the model, as above.\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model2 = TwoLayerLinearRegressor(3, 1)\n","model2.to(device)\n","\n","train(model2, data_train, data_val, device)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"priZSqzc1wrW"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ground Truth:\n"," [ 0.02984931 -2.37607964 -1.10216844 -0.0546588   2.05416286  2.26763019\n","  2.12402512 -4.33808936  5.42576936  1.55617834  1.47426999 -1.17686426\n"," -4.17001831 -4.92588359 -3.1593322 ]\n","Model Output:\n"," [tensor([0.0389], grad_fn=<ViewBackward0>), tensor([-2.3715], grad_fn=<ViewBackward0>), tensor([-1.1023], grad_fn=<ViewBackward0>), tensor([-0.0507], grad_fn=<ViewBackward0>), tensor([2.0471], grad_fn=<ViewBackward0>), tensor([2.2552], grad_fn=<ViewBackward0>), tensor([2.1269], grad_fn=<ViewBackward0>), tensor([-4.3342], grad_fn=<ViewBackward0>), tensor([5.4154], grad_fn=<ViewBackward0>), tensor([1.5548], grad_fn=<ViewBackward0>), tensor([1.4816], grad_fn=<ViewBackward0>), tensor([-1.1757], grad_fn=<ViewBackward0>), tensor([-4.1606], grad_fn=<ViewBackward0>), tensor([-4.9090], grad_fn=<ViewBackward0>), tensor([-3.1481], grad_fn=<ViewBackward0>)]\n"]}],"source":["# Let's see how well this model agrees with the training data\n","output_values = evaluate_model(model2,data_val,device)\n","print('Ground Truth:\\n',Y)\n","print('Model Output:\\n',output_values)"]},{"cell_type":"markdown","metadata":{"id":"d-oOdWV8C8Nu"},"source":["# The XOR Data Set\n","We see that linear layers do not suffice."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"8Qk0d0DUzt1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["These are the labels:\n"," [0 1 1 0]\n","These are the features:\n"," [[0 0]\n"," [0 1]\n"," [1 0]\n"," [1 1]]\n"]}],"source":["\"\"\"\n","Here we create the simple XOR data set an a numpy array.\n","Then we make X and Y into tensor objects that torch uses,\n","and we package it into a Dataset object called data.\n","Then we create a DataLoader.\n","\"\"\"\n","\n","Xxor = np.array([[0,0],[0,1],[1,0],[1,1]])\n","Yxor = np.array([0,1,1,0])\n","tensor_xxor = torch.Tensor(Xxor) # transform to torch tensor\n","tensor_yxor = torch.Tensor(Yxor)\n","print('These are the labels:\\n',Yxor)\n","print('These are the features:\\n',Xxor)\n","\n","dataxor = TensorDataset(tensor_xxor,tensor_yxor) # create your datset\n","dataxor_train = DataLoader(dataxor) # create your dataloader with training data\n","dataxor_val = DataLoader(dataxor) # create your dataloader with validation data, here same as training"]},{"cell_type":"markdown","metadata":{"id":"CZngJgatnmr2"},"source":["## Problem 9\n","\n","Train your linear regressor on these data. Now see how well you do, by evaluating your solution on the training data.\n","\n","Print your output. Do you get the right values?"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"8KJ5-QW8-q1I"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"540bcc7e0b48452887cafd814b6ccf08","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Ground Truth:\n"," [0 1 1 0]\n","Model Output:\n"," [tensor([0.5012], grad_fn=<ViewBackward0>), tensor([0.5006], grad_fn=<ViewBackward0>), tensor([0.5005], grad_fn=<ViewBackward0>), tensor([0.4999], grad_fn=<ViewBackward0>)]\n"]}],"source":["# Now we train a linear classifier on these data.\n","# We know (and can verify) that this will fail because no linear classifier can succeed\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model3 = LinearRegressor(2, 1)# TO DO\n","model3.to(device)\n","\n","# TO DO -- give a command to train your model\n","train(model3, dataxor_train, dataxor_val, device)\n","# TO DO -- give a command to evaluate your model\n","output_values = evaluate_model(model3, dataxor_val, device)\n","\n","# TO DO -- print the ground truth, and then also print what your model predicts for the 4 points in the training set.\n","print('Ground Truth:\\n',Yxor)\n","print('Model Output:\\n',output_values)"]},{"cell_type":"markdown","metadata":{"id":"xUuz5mQY41j0"},"source":["## Problem 10\n","\n","Now repeat this, but using both versions of your non-linear two-layer model. Thus: train both versions of your non-linear two layer models, and evaluate them on the data.\n","\n","Remember the values we got in class. We saw that there is a solution -- i.e., a setting for the parameters of the nonlinear model -- so that the network outputs the XOR outputs.\n","\n","If you got the right values, try a different random seed.\n","If you didn't get the right values, try changing the random seed to see if you get something different."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f53c547b57d34e57859b47e8e0c13645","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d5c40ddefc746c5922db1e5451f4e56","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Ground Truth:\n"," [0 1 1 0]\n","Model 4 Output:\n"," [tensor([0.3338], grad_fn=<ViewBackward0>), tensor([1.0007], grad_fn=<ViewBackward0>), tensor([0.3338], grad_fn=<ViewBackward0>), tensor([0.3338], grad_fn=<ViewBackward0>)]\n","Model 5 Output:\n"," [tensor([0.5005], grad_fn=<ViewBackward0>), tensor([0.5005], grad_fn=<ViewBackward0>), tensor([0.5005], grad_fn=<ViewBackward0>), tensor([0.5005], grad_fn=<ViewBackward0>)]\n"]}],"source":["import random\n","\n","# Set the random seed for reproducibility\n","seed = 0\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","model4 = TwoLayerNonLinearRegressor(2, 1)\n","model5 = TwoLayerNonLinearRegressor2(2, 1)\n","\n","model4.to(device)\n","model5.to(device)\n","\n","train(model4, dataxor_train, dataxor_val, device)\n","train(model5, dataxor_train, dataxor_val, device)\n","\n","output_values = evaluate_model(model4, dataxor_val, device)\n","\n","print('Ground Truth:\\n',Yxor)\n","print('Model 4 Output:\\n',output_values)\n","\n","output_values = evaluate_model(model5, dataxor_val, device)\n","print('Model 5 Output:\\n',output_values)"]},{"cell_type":"markdown","metadata":{"id":"OiONq3mb4_9I"},"source":["## Problem 11\n","\n","If you did the above correctly, you will see that the values depend quite a bit on the random seed. In fact, you probably noticed that most random seeds **don't seem to work**.\n","\n","What is happening here is that we are getting stuck in a local minimum, and our gradient-based method used in the training loop cannot escape from it.\n","\n","Modify the definition of your non-linear two layer network so that the first layer of neurons (the hidden layer) has more than 2 (say, 10 or 15) neurons. Note that this means your network has more parameters.\n","\n","Then run it again with different seeds on the XOR training data. Is it easier (i.e., easier to find a random seed that works)?"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a7c0f70d62e4bfda3a5e53c84cdacd0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Ground Truth:\n"," [0 1 1 0]\n","Model 6 Output:\n"," [tensor([0.], grad_fn=<ViewBackward0>), tensor([1.], grad_fn=<ViewBackward0>), tensor([1.], grad_fn=<ViewBackward0>), tensor([0.], grad_fn=<ViewBackward0>)]\n"]}],"source":["model6 = TwoLayerNonLinearRegressor(2, 1)\n","\n","model6.to(device)\n","\n","train(model6, dataxor_train, dataxor_val, device)\n","\n","output_values = evaluate_model(model6, dataxor_val, device)\n","\n","print('Ground Truth:\\n',Yxor)\n","print('Model 6 Output:\\n',output_values)\n"]},{"cell_type":"markdown","metadata":{"id":"NPRZcvQbpL6K"},"source":["## Problem 12\n","\n","Print the parameters of one of your non-linear models. Thus, you should print: 4 weights plus 2 bias values for the first layer, and then 2 weights plus 1 bias value for the second: 9 parameters in total."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"dK_N_j31wCD1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model parameters:\n","linear.0.weight: tensor([[-0.8780,  1.0555],\n","        [-0.5820, -0.5204]])\n","linear.0.bias: tensor([-0.2263, -0.0635])\n","linear.2.weight: tensor([[0.8042, 0.3560]])\n","linear.2.bias: tensor([0.3338])\n"]}],"source":["# Print the parameters\n","print(\"Model parameters:\")\n","for name, param in model4.named_parameters():\n","    print(f\"{name}: {param.data}\")"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
